  0 KSP Residual norm 812.472 
  1 KSP Residual norm 233.701 
  2 KSP Residual norm 104.667 
  3 KSP Residual norm 21.2883 
  4 KSP Residual norm 7.08074 
  5 KSP Residual norm 2.43076 
  6 KSP Residual norm 1.07247 
  7 KSP Residual norm 0.590685 
  8 KSP Residual norm 0.187347 
  9 KSP Residual norm 0.0475503 
 10 KSP Residual norm 0.0106777 
 11 KSP Residual norm 0.0016059 
Linear solve converged due to CONVERGED_RTOL iterations 11
KSP Object: 8 MPI processes
  type: cg
  maximum iterations=10000, initial guess is zero
  tolerances:  relative=1e-05, absolute=1e-50, divergence=10000.
  left preconditioning
  using PRECONDITIONED norm type for convergence test
PC Object: 8 MPI processes
  type: gamg
    type is MULTIPLICATIVE, levels=2 cycles=v
      Cycles per PCApply=1
      Using externally compute Galerkin coarse grid matrices
      GAMG specific options
        Threshold for dropping small values in graph on each level =  
        Threshold scaling factor for each level not specified = 1.
        Using aggregates from coarsening process to define subdomains for PCASM
        Using parallel coarse grid solver (all coarse grid equations not put on one process)
        AGG specific options
          Symmetric graph false
          Number of levels to square graph 1
          Number smoothing steps 1
  Coarse grid solver -- level -------------------------------
    KSP Object: (mg_coarse_) 8 MPI processes
      type: cg
      maximum iterations=10000, initial guess is zero
      tolerances:  relative=1e-05, absolute=1e-50, divergence=10000.
      left preconditioning
      using PRECONDITIONED norm type for convergence test
    PC Object: (mg_coarse_) 8 MPI processes
      type: jacobi
      linear system matrix = precond matrix:
      Mat Object: 8 MPI processes
        type: mpiaij
        rows=162, cols=162, bs=6
        total: nonzeros=14076, allocated nonzeros=14076
        total number of mallocs used during MatSetValues calls =0
          using I-node (on process 0) routines: found 4 nodes, limit used is 5
  Down solver (pre-smoother) on level 1 -------------------------------
    KSP Object: (mg_levels_1_) 8 MPI processes
      type: chebyshev
        eigenvalue estimates used:  min = 0.451919, max = 2.37258
        eigenvalues estimate via cg min 0.032078, max 2.2596
        eigenvalues estimated using cg with translations  [0. 0.2; 0. 1.05]
        KSP Object: (mg_levels_1_esteig_) 8 MPI processes
          type: cg
          maximum iterations=10, initial guess is zero
          tolerances:  relative=1e-12, absolute=1e-50, divergence=10000.
          left preconditioning
          using PRECONDITIONED norm type for convergence test
        estimating eigenvalues using noisy right hand side
      maximum iterations=1, nonzero initial guess
      tolerances:  relative=1e-05, absolute=1e-50, divergence=10000.
      left preconditioning
      using NONE norm type for convergence test
    PC Object: (mg_levels_1_) 8 MPI processes
      type: asm
        total subdomain blocks = 27, amount of overlap = 0
        restriction/interpolation type - BASIC
        Local solve is same for all blocks, in the following KSP and PC objects:
      KSP Object: (mg_levels_1_sub_) 1 MPI processes
        type: preonly
        maximum iterations=10000, initial guess is zero
        tolerances:  relative=1e-05, absolute=1e-50, divergence=10000.
        left preconditioning
        using NONE norm type for convergence test
      PC Object: (mg_levels_1_sub_) 1 MPI processes
        type: lu
          out-of-place factorization
          tolerance for zero pivot 2.22045e-14
          matrix ordering: nd
          factor fill ratio given 5., needed 1.43328
            Factored matrix follows:
              Mat Object: 1 MPI processes
                type: seqaij
                rows=135, cols=135
                package used to perform factorization: petsc
                total: nonzeros=8217, allocated nonzeros=8217
                total number of mallocs used during MatSetValues calls =0
                  using I-node routines: found 45 nodes, limit used is 5
        linear system matrix = precond matrix:
        Mat Object: 1 MPI processes
          type: seqaij
          rows=135, cols=135
          total: nonzeros=5733, allocated nonzeros=5733
          total number of mallocs used during MatSetValues calls =0
            using I-node routines: found 45 nodes, limit used is 5
      linear system matrix = precond matrix:
      Mat Object: 8 MPI processes
        type: mpiaij
        rows=3000, cols=3000, bs=3
        total: nonzeros=197568, allocated nonzeros=243000
        total number of mallocs used during MatSetValues calls =0
          has attached near null space
          using I-node (on process 0) routines: found 125 nodes, limit used is 5
  Up solver (post-smoother) same as down solver (pre-smoother)
  linear system matrix = precond matrix:
  Mat Object: 8 MPI processes
    type: mpiaij
    rows=3000, cols=3000, bs=3
    total: nonzeros=197568, allocated nonzeros=243000
    total number of mallocs used during MatSetValues calls =0
      has attached near null space
      using I-node (on process 0) routines: found 125 nodes, limit used is 5
  0 KSP Residual norm 0.00812472 
  1 KSP Residual norm 0.00233701 
  2 KSP Residual norm 0.00104667 
  3 KSP Residual norm 0.000212883 
  4 KSP Residual norm 7.08074e-05 
  5 KSP Residual norm 2.43076e-05 
  6 KSP Residual norm 1.07247e-05 
  7 KSP Residual norm 5.90685e-06 
  8 KSP Residual norm 1.87347e-06 
  9 KSP Residual norm 4.75503e-07 
 10 KSP Residual norm 1.06777e-07 
 11 KSP Residual norm 1.6059e-08 
Linear solve converged due to CONVERGED_RTOL iterations 11
KSP Object: 8 MPI processes
  type: cg
  maximum iterations=10000, initial guess is zero
  tolerances:  relative=1e-05, absolute=1e-50, divergence=10000.
  left preconditioning
  using PRECONDITIONED norm type for convergence test
PC Object: 8 MPI processes
  type: gamg
    type is MULTIPLICATIVE, levels=2 cycles=v
      Cycles per PCApply=1
      Using externally compute Galerkin coarse grid matrices
      GAMG specific options
        Threshold for dropping small values in graph on each level =  
        Threshold scaling factor for each level not specified = 1.
        Using aggregates from coarsening process to define subdomains for PCASM
        Using parallel coarse grid solver (all coarse grid equations not put on one process)
        AGG specific options
          Symmetric graph false
          Number of levels to square graph 1
          Number smoothing steps 1
  Coarse grid solver -- level -------------------------------
    KSP Object: (mg_coarse_) 8 MPI processes
      type: cg
      maximum iterations=10000, initial guess is zero
      tolerances:  relative=1e-05, absolute=1e-50, divergence=10000.
      left preconditioning
      using PRECONDITIONED norm type for convergence test
    PC Object: (mg_coarse_) 8 MPI processes
      type: jacobi
      linear system matrix = precond matrix:
      Mat Object: 8 MPI processes
        type: mpiaij
        rows=162, cols=162, bs=6
        total: nonzeros=14076, allocated nonzeros=14076
        total number of mallocs used during MatSetValues calls =0
          using nonscalable MatPtAP() implementation
          using I-node (on process 0) routines: found 4 nodes, limit used is 5
  Down solver (pre-smoother) on level 1 -------------------------------
    KSP Object: (mg_levels_1_) 8 MPI processes
      type: chebyshev
        eigenvalue estimates used:  min = 0.451919, max = 2.37258
        eigenvalues estimate via cg min 0.032078, max 2.2596
        eigenvalues estimated using cg with translations  [0. 0.2; 0. 1.05]
        KSP Object: (mg_levels_1_esteig_) 8 MPI processes
          type: cg
          maximum iterations=10, initial guess is zero
          tolerances:  relative=1e-12, absolute=1e-50, divergence=10000.
          left preconditioning
          using PRECONDITIONED norm type for convergence test
        estimating eigenvalues using noisy right hand side
      maximum iterations=1, nonzero initial guess
      tolerances:  relative=1e-05, absolute=1e-50, divergence=10000.
      left preconditioning
      using NONE norm type for convergence test
    PC Object: (mg_levels_1_) 8 MPI processes
      type: asm
        total subdomain blocks = 27, amount of overlap = 0
        restriction/interpolation type - BASIC
        Local solve is same for all blocks, in the following KSP and PC objects:
      KSP Object: (mg_levels_1_sub_) 1 MPI processes
        type: preonly
        maximum iterations=10000, initial guess is zero
        tolerances:  relative=1e-05, absolute=1e-50, divergence=10000.
        left preconditioning
        using NONE norm type for convergence test
      PC Object: (mg_levels_1_sub_) 1 MPI processes
        type: lu
          out-of-place factorization
          tolerance for zero pivot 2.22045e-14
          matrix ordering: nd
          factor fill ratio given 5., needed 1.43328
            Factored matrix follows:
              Mat Object: 1 MPI processes
                type: seqaij
                rows=135, cols=135
                package used to perform factorization: petsc
                total: nonzeros=8217, allocated nonzeros=8217
                total number of mallocs used during MatSetValues calls =0
                  using I-node routines: found 45 nodes, limit used is 5
        linear system matrix = precond matrix:
        Mat Object: 1 MPI processes
          type: seqaij
          rows=135, cols=135
          total: nonzeros=5733, allocated nonzeros=5733
          total number of mallocs used during MatSetValues calls =0
            using I-node routines: found 45 nodes, limit used is 5
      linear system matrix = precond matrix:
      Mat Object: 8 MPI processes
        type: mpiaij
        rows=3000, cols=3000, bs=3
        total: nonzeros=197568, allocated nonzeros=243000
        total number of mallocs used during MatSetValues calls =0
          has attached near null space
          using I-node (on process 0) routines: found 125 nodes, limit used is 5
  Up solver (post-smoother) same as down solver (pre-smoother)
  linear system matrix = precond matrix:
  Mat Object: 8 MPI processes
    type: mpiaij
    rows=3000, cols=3000, bs=3
    total: nonzeros=197568, allocated nonzeros=243000
    total number of mallocs used during MatSetValues calls =0
      has attached near null space
      using I-node (on process 0) routines: found 125 nodes, limit used is 5
  0 KSP Residual norm 0.00812472 
  1 KSP Residual norm 0.00233701 
  2 KSP Residual norm 0.00104667 
  3 KSP Residual norm 0.000212883 
  4 KSP Residual norm 7.08074e-05 
  5 KSP Residual norm 2.43076e-05 
  6 KSP Residual norm 1.07247e-05 
  7 KSP Residual norm 5.90685e-06 
  8 KSP Residual norm 1.87347e-06 
  9 KSP Residual norm 4.75503e-07 
 10 KSP Residual norm 1.06777e-07 
 11 KSP Residual norm 1.6059e-08 
Linear solve converged due to CONVERGED_RTOL iterations 11
KSP Object: 8 MPI processes
  type: cg
  maximum iterations=10000, initial guess is zero
  tolerances:  relative=1e-05, absolute=1e-50, divergence=10000.
  left preconditioning
  using PRECONDITIONED norm type for convergence test
PC Object: 8 MPI processes
  type: gamg
    type is MULTIPLICATIVE, levels=2 cycles=v
      Cycles per PCApply=1
      Using externally compute Galerkin coarse grid matrices
      GAMG specific options
        Threshold for dropping small values in graph on each level =  
        Threshold scaling factor for each level not specified = 1.
        Using aggregates from coarsening process to define subdomains for PCASM
        Using parallel coarse grid solver (all coarse grid equations not put on one process)
        AGG specific options
          Symmetric graph false
          Number of levels to square graph 1
          Number smoothing steps 1
  Coarse grid solver -- level -------------------------------
    KSP Object: (mg_coarse_) 8 MPI processes
      type: cg
      maximum iterations=10000, initial guess is zero
      tolerances:  relative=1e-05, absolute=1e-50, divergence=10000.
      left preconditioning
      using PRECONDITIONED norm type for convergence test
    PC Object: (mg_coarse_) 8 MPI processes
      type: jacobi
      linear system matrix = precond matrix:
      Mat Object: 8 MPI processes
        type: mpiaij
        rows=162, cols=162, bs=6
        total: nonzeros=14076, allocated nonzeros=14076
        total number of mallocs used during MatSetValues calls =0
          using nonscalable MatPtAP() implementation
          using I-node (on process 0) routines: found 4 nodes, limit used is 5
  Down solver (pre-smoother) on level 1 -------------------------------
    KSP Object: (mg_levels_1_) 8 MPI processes
      type: chebyshev
        eigenvalue estimates used:  min = 0.451919, max = 2.37258
        eigenvalues estimate via cg min 0.032078, max 2.2596
        eigenvalues estimated using cg with translations  [0. 0.2; 0. 1.05]
        KSP Object: (mg_levels_1_esteig_) 8 MPI processes
          type: cg
          maximum iterations=10, initial guess is zero
          tolerances:  relative=1e-12, absolute=1e-50, divergence=10000.
          left preconditioning
          using PRECONDITIONED norm type for convergence test
        estimating eigenvalues using noisy right hand side
      maximum iterations=1, nonzero initial guess
      tolerances:  relative=1e-05, absolute=1e-50, divergence=10000.
      left preconditioning
      using NONE norm type for convergence test
    PC Object: (mg_levels_1_) 8 MPI processes
      type: asm
        total subdomain blocks = 27, amount of overlap = 0
        restriction/interpolation type - BASIC
        Local solve is same for all blocks, in the following KSP and PC objects:
      KSP Object: (mg_levels_1_sub_) 1 MPI processes
        type: preonly
        maximum iterations=10000, initial guess is zero
        tolerances:  relative=1e-05, absolute=1e-50, divergence=10000.
        left preconditioning
        using NONE norm type for convergence test
      PC Object: (mg_levels_1_sub_) 1 MPI processes
        type: lu
          out-of-place factorization
          tolerance for zero pivot 2.22045e-14
          matrix ordering: nd
          factor fill ratio given 5., needed 1.43328
            Factored matrix follows:
              Mat Object: 1 MPI processes
                type: seqaij
                rows=135, cols=135
                package used to perform factorization: petsc
                total: nonzeros=8217, allocated nonzeros=8217
                total number of mallocs used during MatSetValues calls =0
                  using I-node routines: found 45 nodes, limit used is 5
        linear system matrix = precond matrix:
        Mat Object: 1 MPI processes
          type: seqaij
          rows=135, cols=135
          total: nonzeros=5733, allocated nonzeros=5733
          total number of mallocs used during MatSetValues calls =0
            using I-node routines: found 45 nodes, limit used is 5
      linear system matrix = precond matrix:
      Mat Object: 8 MPI processes
        type: mpiaij
        rows=3000, cols=3000, bs=3
        total: nonzeros=197568, allocated nonzeros=243000
        total number of mallocs used during MatSetValues calls =0
          has attached near null space
          using I-node (on process 0) routines: found 125 nodes, limit used is 5
  Up solver (post-smoother) same as down solver (pre-smoother)
  linear system matrix = precond matrix:
  Mat Object: 8 MPI processes
    type: mpiaij
    rows=3000, cols=3000, bs=3
    total: nonzeros=197568, allocated nonzeros=243000
    total number of mallocs used during MatSetValues calls =0
      has attached near null space
      using I-node (on process 0) routines: found 125 nodes, limit used is 5
[0]main |b-Ax|/|b|=3.198358e-05, |b|=5.392179e+00, emax=9.970543e-01
